// Code generated by command: go run src.go -out ../../and_amd64.s -stubs ../../and_stubs_amd64.go -pkg and. DO NOT EDIT.

#include "textflag.h"

// func andAVX2(dst *byte, a *byte, b *byte, l uint64)
// Requires: AVX, AVX2
TEXT ·andAVX2(SB), NOSPLIT, $0-32
	MOVQ a+8(FP), AX
	MOVQ b+16(FP), CX
	MOVQ dst+0(FP), DX
	MOVQ l+24(FP), BX

loop:
	VMOVDQU (AX), Y0
	VMOVDQU (CX), Y8
	VMOVDQU 32(AX), Y1
	VMOVDQU 32(CX), Y9
	VMOVDQU 64(AX), Y2
	VMOVDQU 64(CX), Y10
	VMOVDQU 96(AX), Y3
	VMOVDQU 96(CX), Y11
	VMOVDQU 128(AX), Y4
	VMOVDQU 128(CX), Y12
	VMOVDQU 160(AX), Y5
	VMOVDQU 160(CX), Y13
	VMOVDQU 192(AX), Y6
	VMOVDQU 192(CX), Y14
	VMOVDQU 224(AX), Y7
	VMOVDQU 224(CX), Y15
	VPAND   Y8, Y0, Y8
	VPAND   Y9, Y1, Y9
	VPAND   Y10, Y2, Y10
	VPAND   Y11, Y3, Y11
	VPAND   Y12, Y4, Y12
	VPAND   Y13, Y5, Y13
	VPAND   Y14, Y6, Y14
	VPAND   Y15, Y7, Y15
	VMOVDQU Y8, (DX)
	VMOVDQU Y9, 32(DX)
	VMOVDQU Y10, 64(DX)
	VMOVDQU Y11, 96(DX)
	VMOVDQU Y12, 128(DX)
	VMOVDQU Y13, 160(DX)
	VMOVDQU Y14, 192(DX)
	VMOVDQU Y15, 224(DX)
	ADDQ    $0x00000100, AX
	ADDQ    $0x00000100, CX
	ADDQ    $0x00000100, DX
	SUBQ    $0x00000001, BX
	JNZ     loop
	RET

// func orAVX2(dst *byte, a *byte, b *byte, l uint64)
// Requires: AVX, AVX2
TEXT ·orAVX2(SB), NOSPLIT, $0-32
	MOVQ a+8(FP), AX
	MOVQ b+16(FP), CX
	MOVQ dst+0(FP), DX
	MOVQ l+24(FP), BX

loop:
	VMOVDQU (AX), Y0
	VMOVDQU (CX), Y8
	VMOVDQU 32(AX), Y1
	VMOVDQU 32(CX), Y9
	VMOVDQU 64(AX), Y2
	VMOVDQU 64(CX), Y10
	VMOVDQU 96(AX), Y3
	VMOVDQU 96(CX), Y11
	VMOVDQU 128(AX), Y4
	VMOVDQU 128(CX), Y12
	VMOVDQU 160(AX), Y5
	VMOVDQU 160(CX), Y13
	VMOVDQU 192(AX), Y6
	VMOVDQU 192(CX), Y14
	VMOVDQU 224(AX), Y7
	VMOVDQU 224(CX), Y15
	VPOR    Y8, Y0, Y8
	VPOR    Y9, Y1, Y9
	VPOR    Y10, Y2, Y10
	VPOR    Y11, Y3, Y11
	VPOR    Y12, Y4, Y12
	VPOR    Y13, Y5, Y13
	VPOR    Y14, Y6, Y14
	VPOR    Y15, Y7, Y15
	VMOVDQU Y8, (DX)
	VMOVDQU Y9, 32(DX)
	VMOVDQU Y10, 64(DX)
	VMOVDQU Y11, 96(DX)
	VMOVDQU Y12, 128(DX)
	VMOVDQU Y13, 160(DX)
	VMOVDQU Y14, 192(DX)
	VMOVDQU Y15, 224(DX)
	ADDQ    $0x00000100, AX
	ADDQ    $0x00000100, CX
	ADDQ    $0x00000100, DX
	SUBQ    $0x00000001, BX
	JNZ     loop
	RET

// func andNotAVX2(dst *byte, a *byte, b *byte, l uint64)
// Requires: AVX, AVX2
TEXT ·andNotAVX2(SB), NOSPLIT, $0-32
	MOVQ a+8(FP), AX
	MOVQ b+16(FP), CX
	MOVQ dst+0(FP), DX
	MOVQ l+24(FP), BX

loop:
	VMOVDQU (AX), Y0
	VMOVDQU (CX), Y8
	VMOVDQU 32(AX), Y1
	VMOVDQU 32(CX), Y9
	VMOVDQU 64(AX), Y2
	VMOVDQU 64(CX), Y10
	VMOVDQU 96(AX), Y3
	VMOVDQU 96(CX), Y11
	VMOVDQU 128(AX), Y4
	VMOVDQU 128(CX), Y12
	VMOVDQU 160(AX), Y5
	VMOVDQU 160(CX), Y13
	VMOVDQU 192(AX), Y6
	VMOVDQU 192(CX), Y14
	VMOVDQU 224(AX), Y7
	VMOVDQU 224(CX), Y15
	VPANDN  Y8, Y0, Y8
	VPANDN  Y9, Y1, Y9
	VPANDN  Y10, Y2, Y10
	VPANDN  Y11, Y3, Y11
	VPANDN  Y12, Y4, Y12
	VPANDN  Y13, Y5, Y13
	VPANDN  Y14, Y6, Y14
	VPANDN  Y15, Y7, Y15
	VMOVDQU Y8, (DX)
	VMOVDQU Y9, 32(DX)
	VMOVDQU Y10, 64(DX)
	VMOVDQU Y11, 96(DX)
	VMOVDQU Y12, 128(DX)
	VMOVDQU Y13, 160(DX)
	VMOVDQU Y14, 192(DX)
	VMOVDQU Y15, 224(DX)
	ADDQ    $0x00000100, AX
	ADDQ    $0x00000100, CX
	ADDQ    $0x00000100, DX
	SUBQ    $0x00000001, BX
	JNZ     loop
	RET
