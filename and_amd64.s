// Code generated by command: go run src.go -out ../../and_amd64.s -stubs ../../and_stubs_amd64.go -pkg and. DO NOT EDIT.

#include "textflag.h"

// func andAVX2(dst *byte, a *byte, b *byte, l uint64)
// Requires: AVX, AVX2
TEXT ·andAVX2(SB), NOSPLIT, $0-32
	MOVQ a+8(FP), AX
	MOVQ b+16(FP), CX
	MOVQ dst+0(FP), DX
	MOVQ l+24(FP), BX

loop:
	VMOVDQU (AX), Y0
	VMOVDQU (CX), Y8
	VMOVDQU 32(AX), Y1
	VMOVDQU 32(CX), Y9
	VMOVDQU 64(AX), Y2
	VMOVDQU 64(CX), Y10
	VMOVDQU 96(AX), Y3
	VMOVDQU 96(CX), Y11
	VMOVDQU 128(AX), Y4
	VMOVDQU 128(CX), Y12
	VMOVDQU 160(AX), Y5
	VMOVDQU 160(CX), Y13
	VMOVDQU 192(AX), Y6
	VMOVDQU 192(CX), Y14
	VMOVDQU 224(AX), Y7
	VMOVDQU 224(CX), Y15
	VPAND   Y8, Y0, Y8
	VPAND   Y9, Y1, Y9
	VPAND   Y10, Y2, Y10
	VPAND   Y11, Y3, Y11
	VPAND   Y12, Y4, Y12
	VPAND   Y13, Y5, Y13
	VPAND   Y14, Y6, Y14
	VPAND   Y15, Y7, Y15
	VMOVDQU Y8, (DX)
	VMOVDQU Y9, 32(DX)
	VMOVDQU Y10, 64(DX)
	VMOVDQU Y11, 96(DX)
	VMOVDQU Y12, 128(DX)
	VMOVDQU Y13, 160(DX)
	VMOVDQU Y14, 192(DX)
	VMOVDQU Y15, 224(DX)
	ADDQ    $0x00000100, AX
	ADDQ    $0x00000100, CX
	ADDQ    $0x00000100, DX
	SUBQ    $0x00000001, BX
	JNZ     loop
	RET

// func orAVX2(dst *byte, a *byte, b *byte, l uint64)
// Requires: AVX, AVX2
TEXT ·orAVX2(SB), NOSPLIT, $0-32
	MOVQ a+8(FP), AX
	MOVQ b+16(FP), CX
	MOVQ dst+0(FP), DX
	MOVQ l+24(FP), BX

loop:
	VMOVDQU (AX), Y0
	VMOVDQU (CX), Y8
	VMOVDQU 32(AX), Y1
	VMOVDQU 32(CX), Y9
	VMOVDQU 64(AX), Y2
	VMOVDQU 64(CX), Y10
	VMOVDQU 96(AX), Y3
	VMOVDQU 96(CX), Y11
	VMOVDQU 128(AX), Y4
	VMOVDQU 128(CX), Y12
	VMOVDQU 160(AX), Y5
	VMOVDQU 160(CX), Y13
	VMOVDQU 192(AX), Y6
	VMOVDQU 192(CX), Y14
	VMOVDQU 224(AX), Y7
	VMOVDQU 224(CX), Y15
	VPOR    Y8, Y0, Y8
	VPOR    Y9, Y1, Y9
	VPOR    Y10, Y2, Y10
	VPOR    Y11, Y3, Y11
	VPOR    Y12, Y4, Y12
	VPOR    Y13, Y5, Y13
	VPOR    Y14, Y6, Y14
	VPOR    Y15, Y7, Y15
	VMOVDQU Y8, (DX)
	VMOVDQU Y9, 32(DX)
	VMOVDQU Y10, 64(DX)
	VMOVDQU Y11, 96(DX)
	VMOVDQU Y12, 128(DX)
	VMOVDQU Y13, 160(DX)
	VMOVDQU Y14, 192(DX)
	VMOVDQU Y15, 224(DX)
	ADDQ    $0x00000100, AX
	ADDQ    $0x00000100, CX
	ADDQ    $0x00000100, DX
	SUBQ    $0x00000001, BX
	JNZ     loop
	RET

// func andNotAVX2(dst *byte, a *byte, b *byte, l uint64)
// Requires: AVX, AVX2
TEXT ·andNotAVX2(SB), NOSPLIT, $0-32
	MOVQ a+8(FP), AX
	MOVQ b+16(FP), CX
	MOVQ dst+0(FP), DX
	MOVQ l+24(FP), BX

loop:
	VMOVDQU (AX), Y0
	VMOVDQU (CX), Y8
	VMOVDQU 32(AX), Y1
	VMOVDQU 32(CX), Y9
	VMOVDQU 64(AX), Y2
	VMOVDQU 64(CX), Y10
	VMOVDQU 96(AX), Y3
	VMOVDQU 96(CX), Y11
	VMOVDQU 128(AX), Y4
	VMOVDQU 128(CX), Y12
	VMOVDQU 160(AX), Y5
	VMOVDQU 160(CX), Y13
	VMOVDQU 192(AX), Y6
	VMOVDQU 192(CX), Y14
	VMOVDQU 224(AX), Y7
	VMOVDQU 224(CX), Y15
	VPANDN  Y8, Y0, Y8
	VPANDN  Y9, Y1, Y9
	VPANDN  Y10, Y2, Y10
	VPANDN  Y11, Y3, Y11
	VPANDN  Y12, Y4, Y12
	VPANDN  Y13, Y5, Y13
	VPANDN  Y14, Y6, Y14
	VPANDN  Y15, Y7, Y15
	VMOVDQU Y8, (DX)
	VMOVDQU Y9, 32(DX)
	VMOVDQU Y10, 64(DX)
	VMOVDQU Y11, 96(DX)
	VMOVDQU Y12, 128(DX)
	VMOVDQU Y13, 160(DX)
	VMOVDQU Y14, 192(DX)
	VMOVDQU Y15, 224(DX)
	ADDQ    $0x00000100, AX
	ADDQ    $0x00000100, CX
	ADDQ    $0x00000100, DX
	SUBQ    $0x00000001, BX
	JNZ     loop
	RET

// func popcntAsm(a *byte, l uint64) int
// Requires: POPCNT
TEXT ·popcntAsm(SB), NOSPLIT, $0-24
	MOVQ a+0(FP), AX
	MOVQ l+8(FP), CX
	XORQ DX, DX

loop:
	MOVQ    (AX), BX
	MOVQ    8(AX), SI
	MOVQ    16(AX), DI
	MOVQ    24(AX), R8
	MOVQ    32(AX), R9
	MOVQ    40(AX), R10
	MOVQ    48(AX), R11
	MOVQ    56(AX), R12
	POPCNTQ BX, BX
	POPCNTQ SI, SI
	POPCNTQ DI, DI
	POPCNTQ R8, R8
	POPCNTQ R9, R9
	POPCNTQ R10, R10
	POPCNTQ R11, R11
	POPCNTQ R12, R12
	ADDQ    BX, DX
	ADDQ    SI, DX
	ADDQ    DI, DX
	ADDQ    R8, DX
	ADDQ    R9, DX
	ADDQ    R10, DX
	ADDQ    R11, DX
	ADDQ    R12, DX
	ADDQ    $0x00000040, AX
	SUBQ    $0x00000001, CX
	JNZ     loop
	MOVQ    DX, ret+16(FP)
	RET

// func memsetAVX2(dst *byte, l uint64, b byte)
// Requires: AVX, AVX2
TEXT ·memsetAVX2(SB), NOSPLIT, $0-17
	MOVQ         dst+0(FP), AX
	MOVQ         l+8(FP), CX
	VPBROADCASTB b+16(FP), Y0

loop:
	VMOVDQU Y0, (AX)
	ADDQ    $0x00000020, AX
	SUBQ    $0x00000001, CX
	JNZ     loop
	RET
