// +build arm64,!purego

#include "textflag.h"

// func andNEON(dst *byte, a *byte, b *byte, l uint64)
TEXT ·andNEON(SB), NOSPLIT, $0-32
    MOVD dst+0(FP), R0
    MOVD a+8(FP), R1
    MOVD b+16(FP), R2
    MOVD l+24(FP), R3

loop:
    VLD1.P 64(R1), [ V0.B16,  V1.B16,  V2.B16,  V3.B16]
    VLD1.P 64(R2), [ V4.B16,  V5.B16,  V6.B16,  V7.B16]
    VLD1.P 64(R1), [ V8.B16,  V9.B16, V10.B16, V11.B16]
    VLD1.P 64(R2), [V12.B16, V13.B16, V14.B16, V15.B16]
    VLD1.P 64(R1), [V16.B16, V17.B16, V18.B16, V19.B16]
    VLD1.P 64(R2), [V20.B16, V21.B16, V22.B16, V23.B16]
    VLD1.P 64(R1), [V24.B16, V25.B16, V26.B16, V27.B16]
    VLD1.P 64(R2), [V28.B16, V29.B16, V30.B16, V31.B16]

    VAND  V0.B16,  V4.B16,  V0.B16
    VAND  V1.B16,  V5.B16,  V1.B16
    VAND  V2.B16,  V6.B16,  V2.B16
    VAND  V3.B16,  V7.B16,  V3.B16

    VAND  V8.B16, V12.B16,  V8.B16
    VAND  V9.B16, V13.B16,  V9.B16
    VAND V10.B16, V14.B16, V10.B16
    VAND V11.B16, V15.B16, V11.B16

    VAND V16.B16, V20.B16, V16.B16
    VAND V17.B16, V21.B16, V17.B16
    VAND V18.B16, V22.B16, V18.B16
    VAND V19.B16, V23.B16, V19.B16

    VAND V24.B16, V28.B16, V24.B16
    VAND V25.B16, V29.B16, V25.B16
    VAND V26.B16, V30.B16, V26.B16
    VAND V27.B16, V31.B16, V27.B16

    VST1.P [ V0.B16,  V1.B16,  V2.B16,  V3.B16], 64(R0)
    VST1.P [ V8.B16,  V9.B16, V10.B16, V11.B16], 64(R0)
    VST1.P [V16.B16, V17.B16, V18.B16, V19.B16], 64(R0)
    VST1.P [V24.B16, V25.B16, V26.B16, V27.B16], 64(R0)

    SUBS $1, R3, R3
    CBNZ R3, loop

    RET

// func orNEON(dst *byte, a *byte, b *byte, l uint64)
TEXT ·orNEON(SB), NOSPLIT, $0-32
    MOVD dst+0(FP), R0
    MOVD a+8(FP), R1
    MOVD b+16(FP), R2
    MOVD l+24(FP), R3

loop:
    VLD1.P 64(R1), [ V0.B16,  V1.B16,  V2.B16,  V3.B16]
    VLD1.P 64(R2), [ V4.B16,  V5.B16,  V6.B16,  V7.B16]
    VLD1.P 64(R1), [ V8.B16,  V9.B16, V10.B16, V11.B16]
    VLD1.P 64(R2), [V12.B16, V13.B16, V14.B16, V15.B16]
    VLD1.P 64(R1), [V16.B16, V17.B16, V18.B16, V19.B16]
    VLD1.P 64(R2), [V20.B16, V21.B16, V22.B16, V23.B16]
    VLD1.P 64(R1), [V24.B16, V25.B16, V26.B16, V27.B16]
    VLD1.P 64(R2), [V28.B16, V29.B16, V30.B16, V31.B16]

    VORR  V0.B16,  V4.B16,  V0.B16
    VORR  V1.B16,  V5.B16,  V1.B16
    VORR  V2.B16,  V6.B16,  V2.B16
    VORR  V3.B16,  V7.B16,  V3.B16

    VORR  V8.B16, V12.B16,  V8.B16
    VORR  V9.B16, V13.B16,  V9.B16
    VORR V10.B16, V14.B16, V10.B16
    VORR V11.B16, V15.B16, V11.B16

    VORR V16.B16, V20.B16, V16.B16
    VORR V17.B16, V21.B16, V17.B16
    VORR V18.B16, V22.B16, V18.B16
    VORR V19.B16, V23.B16, V19.B16

    VORR V24.B16, V28.B16, V24.B16
    VORR V25.B16, V29.B16, V25.B16
    VORR V26.B16, V30.B16, V26.B16
    VORR V27.B16, V31.B16, V27.B16

    VST1.P [ V0.B16,  V1.B16,  V2.B16,  V3.B16], 64(R0)
    VST1.P [ V8.B16,  V9.B16, V10.B16, V11.B16], 64(R0)
    VST1.P [V16.B16, V17.B16, V18.B16, V19.B16], 64(R0)
    VST1.P [V24.B16, V25.B16, V26.B16, V27.B16], 64(R0)

    SUBS $1, R3, R3
    CBNZ R3, loop

    RET

// func popcntNEON(a *byte, l uint64) uint64
TEXT ·popcntNEON(SB), NOSPLIT, $0-24
    MOVD a+0(FP), R1
    MOVD l+8(FP), R2

    VEOR V16.B16, V16.B16, V16.B16 // zero

loop:
    VLD1.P 64(R1), [ V0.B16,  V1.B16,  V2.B16,  V3.B16]
    VLD1.P 64(R1), [ V4.B16,  V5.B16,  V6.B16,  V7.B16]
    VLD1.P 64(R1), [ V8.B16,  V9.B16, V10.B16, V11.B16]
    VLD1.P 64(R1), [V12.B16, V13.B16, V14.B16, V15.B16]

    VCNT  V0.B16,  V0.B16
    VCNT  V1.B16,  V1.B16
    VCNT  V2.B16,  V2.B16
    VCNT  V3.B16,  V3.B16
    VCNT  V4.B16,  V4.B16
    VCNT  V5.B16,  V5.B16
    VCNT  V6.B16,  V6.B16
    VCNT  V7.B16,  V7.B16
    VCNT  V8.B16,  V8.B16
    VCNT  V9.B16,  V9.B16
    VCNT V10.B16, V10.B16
    VCNT V11.B16, V11.B16
    VCNT V12.B16, V12.B16
    VCNT V13.B16, V13.B16
    VCNT V14.B16, V14.B16
    VCNT V15.B16, V15.B16

    VADD  V0.B16,  V1.B16,  V0.B16
    VADD  V2.B16,  V3.B16,  V2.B16
    VADD  V4.B16,  V5.B16,  V4.B16
    VADD  V6.B16,  V7.B16,  V6.B16
    VADD  V8.B16,  V9.B16,  V8.B16
    VADD V10.B16, V11.B16, V10.B16
    VADD V12.B16, V13.B16, V12.B16
    VADD V14.B16, V15.B16, V14.B16

    VADD  V0.B16,  V2.B16,  V0.B16
    VADD  V4.B16,  V6.B16,  V4.B16
    VADD  V8.B16, V10.B16,  V8.B16
    VADD V12.B16, V14.B16, V12.B16

    VADD  V0.B16,  V4.B16,  V0.B16
    VADD  V8.B16, V12.B16,  V8.B16

    VADD  V0.B16,  V8.B16,  V0.B16

    VUADDLV V0.B16, V0
    VADDV V0.H8, V0

    VADD V16.D2, V0.D2, V16.D2

    SUBS $1, R2, R2
    CBNZ R2, loop

    VMOV V16.D[0], R0
    MOVD R0, ret+16(FP)

    RET
